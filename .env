# =================================================================
# ==      CONFIGURATION ARCHITECTURALE POUR STACK GRATUITE       ==
# ==      (NeonDB + Neo4j Aura + Google Gemini)                  ==
# ==      Validée par JabbarRoot-Architecte v4.0                 ==
# =================================================================

# 1. Configuration de la Base de Données (PostgreSQL via Neon)
# Utilisation de votre chaîne de connexion fournie.
DATABASE_URL=postgresql://neondb_owner:npg_QtJy5VS8heAM@ep-dry-band-abt7t5ij-pooler.eu-west-2.aws.neon.tech/neondb?sslmode=require

# 2. Configuration du Knowledge Graph (Neo4j Aura)
# Utilisation de vos identifiants d'instance AuraDB.
# Note: Le nom de la base est 'neo4j' par défaut sur Aura.
NEO4J_URI=neo4j+s://65a02f6c.databases.neo4j.io
NEO4J_USER=neo4j
NEO4J_PASSWORD=dQlZZ_fDco3NxBqGae7iaCf4LiKB1mFsM97lWyZH0tY

# 3. Configuration du Fournisseur LLM (Google Gemini)
# Passage à Gemini pour les capacités de raisonnement de l'agent.
LLM_PROVIDER=google
LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta
LLM_API_KEY=# =================================================================
# ==      CONFIGURATION ARCHITECTURALE POUR STACK GRATUITE       ==
# ==      (NeonDB + Neo4j Aura + Google Gemini)                  ==
# ==      Validée par JabbarRoot-Architecte v4.0                 ==
# =================================================================

# 1. Configuration de la Base de Données (PostgreSQL via Neon)
# Utilisation de votre chaîne de connexion fournie.
DATABASE_URL=postgresql://neondb_owner:npg_QtJy5VS8heAM@ep-dry-band-abt7t5ij-pooler.eu-west-2.aws.neon.tech/neondb?sslmode=require

# 2. Configuration du Knowledge Graph (Neo4j Aura)
# Utilisation de vos identifiants d'instance AuraDB.
# Note: Le nom de la base est 'neo4j' par défaut sur Aura.
NEO4J_URI=neo4j+s://65a02f6c.databases.neo4j.io
NEO4J_USER=neo4j
NEO4J_PASSWORD=dQlZZ_fDco3NxBqGae7iaCf4LiKB1mFsM97lWyZH0tY

# 3. Configuration du Fournisseur LLM (Google Gemini)
# Passage à Gemini pour les capacités de raisonnement de l'agent.
LLM_PROVIDER=google
LLM_BASE_URL=https://generativelanguage.googleapis.com/v1beta
LLM_API_KEY=AIzaSyBwWvwChWuGkGSRDtrLeTXOpoNQJ46aUmc # <-- REMPLACEZ PAR VOTRE VRAIE CLÉ
LLM_CHOICE=gemini-2.5-flash # Modèle performant et économique, idéal pour le free tier.

# 4. Configuration du Fournisseur d'Embedding (Google Gemini)
# Alignement sur Gemini pour la génération des vecteurs.
EMBEDDING_PROVIDER=google
EMBEDDING_BASE_URL=https://generativelanguage.googleapis.com/v1beta
EMBEDDING_API_KEY=AIzaSyBwWvwChWuGkGSRDtrLeTXOpoNQJ46aUmc # <-- UTILISEZ LA MÊME CLÉ QUE CI-DESSUS
EMBEDDING_MODEL=text-embedding-004 # Modèle d'embedding de Google.

# 5. Configuration de l'Ingestion
# Utilisation d'un modèle rapide et économique pour le traitement des documents.
INGESTION_LLM_CHOICE=gemini-1.5-flash # Assez rapide et peu coûteux pour cette tâche.

# 6. Configuration de l'Application
APP_ENV=development
LOG_LEVEL=INFO
APP_PORT=8058

# 7. Configuration du Vector Search (POINT CRUCIAL)
# La dimension doit correspondre au modèle d'embedding 'text-embedding-004'.
VECTOR_DIMENSION=768 # <-- CHANGEMENT CRITIQUE (était 1536 pour OpenAI)
MAX_SEARCH_RESULTS=10

# 8. Autres configurations (valeurs par défaut raisonnables)
CHUNK_SIZE=800
CHUNK_OVERLAP=150
SESSION_TIMEOUT_MINUTES=60
DEBUG_MODE=false # <-- REMPLACEZ PAR VOTRE VRAIE CLÉ
LLM_CHOICE=gemini-2.5-flash # Modèle performant et économique, idéal pour le free tier.

# 4. Configuration du Fournisseur d'Embedding (Google Gemini)
# Alignement sur Gemini pour la génération des vecteurs.
EMBEDDING_PROVIDER=google
EMBEDDING_BASE_URL=https://generativelanguage.googleapis.com/v1beta
EMBEDDING_API_KEY=AIzaSyBwWvwChWuGkGSRDtrLeTXOpoNQJ46aUmc # <-- UTILISEZ LA MÊME CLÉ QUE CI-DESSUS
EMBEDDING_MODEL=text-embedding-004 # Modèle d'embedding de Google.

# 5. Configuration de l'Ingestion
# Utilisation d'un modèle rapide et économique pour le traitement des documents.
INGESTION_LLM_CHOICE=gemini-1.5-flash # Assez rapide et peu coûteux pour cette tâche.

# 6. Configuration de l'Application
APP_ENV=development
LOG_LEVEL=INFO
APP_PORT=8058

# 7. Configuration du Vector Search (POINT CRUCIAL)
# La dimension doit correspondre au modèle d'embedding 'text-embedding-004'.
VECTOR_DIMENSION=768 # <-- CHANGEMENT CRITIQUE (était 1536 pour OpenAI)
MAX_SEARCH_RESULTS=10

# 8. Autres configurations (valeurs par défaut raisonnables)
CHUNK_SIZE=800
CHUNK_OVERLAP=150
SESSION_TIMEOUT_MINUTES=60
DEBUG_MODE=false